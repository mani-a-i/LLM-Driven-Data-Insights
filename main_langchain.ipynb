{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c5a66c5-e77d-4b6a-a015-eec12147c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7b8eea-0342-4ed6-914e-97c745909708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>discipline</th>\n",
       "      <th>phd</th>\n",
       "      <th>service</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prof</td>\n",
       "      <td>B</td>\n",
       "      <td>56</td>\n",
       "      <td>49</td>\n",
       "      <td>Male</td>\n",
       "      <td>186960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prof</td>\n",
       "      <td>A</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>Male</td>\n",
       "      <td>93000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prof</td>\n",
       "      <td>A</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>110515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prof</td>\n",
       "      <td>A</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>131205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prof</td>\n",
       "      <td>B</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>Male</td>\n",
       "      <td>104800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank discipline  phd  service   sex  salary\n",
       "0  Prof          B   56       49  Male  186960\n",
       "1  Prof          A   12        6  Male   93000\n",
       "2  Prof          A   23       20  Male  110515\n",
       "3  Prof          A   40       31  Male  131205\n",
       "4  Prof          B   20       18  Male  104800"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Salaries.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5ea6f9-74c3-4b5a-b29f-e03a9dbaccbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Salaries.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name = 'Salaries.csv'\n",
    "df.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d435ecd-ea7b-4832-811f-cf3ecd3d0517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mgpu-server         \u001b[m  Tue Feb 27 12:55:07 2024  \u001b[1m\u001b[30m495.29.05\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mQuadro RTX 5000 \u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   17\u001b[m / \u001b[33m16122\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m9M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m3M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0046508f-ba98-43e7-9ee2-0ec8f2f4319d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_to_use = 0\n",
    "device = torch.device(f'cuda:{gpu_to_use}' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a7e2d6-be02-453e-93cc-3f17a110f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n",
    "                                          load_in_4bit = True\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "245ab124-832a-45aa-b7b9-112a375c7c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efa0362869e40788b13e946987046fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n",
    "                                             load_in_4bit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "498f7cc8-2a3b-4f3d-9ab8-dd461715ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                top_k = 1,\n",
    "                do_sample = True,\n",
    "                temperature = 0.2,\n",
    "                max_new_tokens = 200,\n",
    "                eos_token_id = [10897,63,4686,10252],\n",
    "                \n",
    "               )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f51f236-3aab-4648-b6f3-1824b19ca7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:10897 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " into a pandas dataframe\n",
      "df = pd.read_csv('file.csv')\n",
      "\n",
      "#write pandas code to import a json file into a pandas dataframe\n",
      "df = pd.read_json('file.json')\n",
      "\n",
      "#write pandas code to import a sql query result into a pandas dataframe\n",
      "import sqlite3\n",
      "conn = sqlite3.connect('my_database.db')\n",
      "df = pd.read_sql_query(\"SELECT * FROM my_table\", conn)\n",
      "\n",
      "#write pandas code to import a sql table into a pandas dataframe\n",
      "df = pd.read_sql_table('my_table', conn)\n",
      "\n",
      "#write pandas code to import a sql query result into a pandas dataframe\n",
      "df = pd.read_sql(\"SELECT * FROM my_table\", conn)\n",
      "\n",
      "#write pandas code to import a sql table into a pandas dataframe\n",
      "df = pd.read_sql_table('my_table', conn)\n",
      "\n",
      "#\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af6daf74-3a84-4c91-8875-50eb208d7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You're a highly skilled Python coder known for your mastery in pandas, seaborn, matplotlib, plotly.\n",
    "When it comes to visualizations, your expertise shines through, crafting aesthetically pleasing plots with captivating color schemes.\n",
    "\n",
    "### Rules:\n",
    "1) print values above the bars of bar plot.\n",
    "2) the output should be phrased with respect to question.\n",
    "3) Use seaborn or plotly along with matplotlib.\n",
    "4) do not print values above the bar if it gets too congested.\n",
    "5) only give the code without explanation and comments.\n",
    "6) Always import necessary libraries and load the dataset.\n",
    "7) Plot only when told.\n",
    "8) Show does not always means you have to plot \n",
    "    # Follow the example below\n",
    "    User: Show me columns of the dataset\n",
    "    Assistant:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(\"Dataset.csv\")\n",
    "    print(df.columns)\n",
    "\n",
    "# You should strictly follow the above rule\n",
    "\n",
    "Here's the scoop on the dataset \"{dataset_name}\" boasting records featuring diverse columns:\n",
    "{dataset_sample}\n",
    "\n",
    "# Follow the example below\n",
    "User: Give me salary distribution across all sex\n",
    "Assistant: \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Salaries.csv\")\n",
    "salary_distribution = df.groupby('sex')['salary'].mean()\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "salary_distribution.plot(kind='bar', color=colors)\n",
    "plt.title('Salary Distribution Across Sexes')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Average Salary')\n",
    "plt.show()\n",
    "\n",
    "# Assist user\n",
    "\n",
    "User: {question}\n",
    "Assistant:\n",
    "```python\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c854b81-8caa-45ea-934e-0d043306c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(prompt)\n",
    "prompt = prompt.partial(dataset_name=df.name,dataset_sample=df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d4d9f2-8e74-44f8-bae9-b53d8d79e0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['dataset_name', 'dataset_sample', 'history', 'input'] template='\\nYou\\'re a highly skilled Python coder known for your mastery in pandas, seaborn, matplotlib, plotly.\\nWhen it comes to visualizations, your expertise shines through, crafting aesthetically pleasing plots with captivating color schemes.\\n\\n### Rules:\\n1) print values above the bars of bar plot.\\n2) the output should be phrased with respect to question.\\n3) Use seaborn or plotly along with matplotlib.\\n4) do not print values above the bar if it gets too congested.\\n5) only give the code without explanation and comments.\\n6) Always import necessary libraries and load the dataset.\\n7) Plot only when told.\\n8) Show does not always means you have to plot \\n    # Follow the example below\\n    User: Show me columns of the dataset\\n    Assistant:\\n    import pandas as pd\\n    df = pd.read_csv(\"Dataset.csv\")\\n    print(df.columns)\\n\\n# You should strictly follow the above rule\\n\\nHere\\'s the scoop on the dataset \"{dataset_name}\" boasting records featuring diverse columns:\\n{dataset_sample}\\n\\n# Follow the example below\\nUser: Give me salary distribution across all sex\\nAssistant: \\nimport pandas as pd\\ndf = pd.read_csv(\"Salaries.csv\")\\nsalary_distribution = df.groupby(\\'sex\\')[\\'salary\\'].mean()\\ncolors = [\\'skyblue\\', \\'lightcoral\\']\\nsalary_distribution.plot(kind=\\'bar\\', color=colors)\\nplt.title(\\'Salary Distribution Across Sexes\\')\\nplt.xlabel(\\'Sex\\')\\nplt.ylabel(\\'Average Salary\\')\\nplt.show()\\n\\nPast conversations:\\n{history}\\n\\n# Assist user\\nHuman: {input}\\nAssistant:\\n```python\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.prompt import PROMPT\n",
    "print(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c89b72f-0286-4e8d-b3d7-1860787ee56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], partial_variables={'dataset_name': 'Salaries.csv', 'dataset_sample':    rank discipline  phd  service   sex  salary\n",
       "0  Prof          B   56       49  Male  186960\n",
       "1  Prof          A   12        6  Male   93000\n",
       "2  Prof          A   23       20  Male  110515\n",
       "3  Prof          A   40       31  Male  131205\n",
       "4  Prof          B   20       18  Male  104800}, template='\\nYou\\'re a highly skilled Python coder known for your mastery in pandas, seaborn, matplotlib, plotly.\\nWhen it comes to visualizations, your expertise shines through, crafting aesthetically pleasing plots with captivating color schemes.\\n\\n### Rules:\\n1) print values above the bars of bar plot.\\n2) the output should be phrased with respect to question.\\n3) Use seaborn or plotly along with matplotlib.\\n4) do not print values above the bar if it gets too congested.\\n5) only give the code without explanation and comments.\\n6) Always import necessary libraries and load the dataset.\\n7) Plot only when told.\\n8) Show does not always means you have to plot \\n    # Follow the example below\\n    User: Show me columns of the dataset\\n    Assistant:\\n    import pandas as pd\\n    df = pd.read_csv(\"Dataset.csv\")\\n    print(df.columns)\\n\\n# You should strictly follow the above rule\\n\\nHere\\'s the scoop on the dataset \"{dataset_name}\" boasting records featuring diverse columns:\\n{dataset_sample}\\n\\n# Follow the example below\\nUser: Give me salary distribution across all sex\\nAssistant: \\nimport pandas as pd\\ndf = pd.read_csv(\"Salaries.csv\")\\nsalary_distribution = df.groupby(\\'sex\\')[\\'salary\\'].mean()\\ncolors = [\\'skyblue\\', \\'lightcoral\\']\\nsalary_distribution.plot(kind=\\'bar\\', color=colors)\\nplt.title(\\'Salary Distribution Across Sexes\\')\\nplt.xlabel(\\'Sex\\')\\nplt.ylabel(\\'Average Salary\\')\\nplt.show()\\n\\nPast conversations:\\n{history}\\n\\n# Assist user\\nHuman: {input}\\nAssistant:\\n```python\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = PROMPT.partial(dataset_name=df.name,dataset_sample=df.head(5))\n",
    "PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b33c20a6-99d9-4285-93b1-4d73a3a8e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory= ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d51b5fc-15ed-4e98-88dc-434cc81d882e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for ConversationChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n__root__\n  Got unexpected prompt input variables. The prompt expects ['dataset_name', 'dataset_sample', 'history', 'input'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[43mConversationChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 3 validation errors for ConversationChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\n__root__\n  Got unexpected prompt input variables. The prompt expects ['dataset_name', 'dataset_sample', 'history', 'input'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=model,\n",
    "                                 memory=memory,\n",
    "                                 verbose=True,\n",
    "                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c07912e-0f95-451f-8e62-a0c41d1ca938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:10897 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "df = pd.read_csv(\"Salaries.csv\")\n",
      "pivot_table = df.pivot_table(values='salary', index='discipline', columns='sex', aggfunc='mean')\n",
      "\n",
      "plt.figure(figsize=(10,6))\n",
      "sns.heatmap(pivot_table, annot=True, fmt=\".0f\", cmap='viridis')\n",
      "plt.title('Average Salaries by Discipline and Gender')\n",
      "plt.show()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "nl_prompt =  prompt.format(question=\"for each gender and discipline plot average salaries\")\n",
    "\n",
    "out = llm.invoke(nl_prompt)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59040e0-d596-4788-964d-aed1238bdecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayur2",
   "language": "python",
   "name": "mayur2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
