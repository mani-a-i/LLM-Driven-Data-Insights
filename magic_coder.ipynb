{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd03da32-4d0b-4634-a760-76e53882862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import (\n",
    "    CombinedMemory,\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    ")\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab9c511-fc62-4f95-a9e2-b1f8fc654329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_to_use = 0\n",
    "device = torch.device(f'cuda:{gpu_to_use}' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7bf87a-ce84-470d-9c4e-6ded147e74d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mgpu-server         \u001b[m  Tue Feb 27 15:17:42 2024  \u001b[1m\u001b[30m495.29.05\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mQuadro RTX 5000 \u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   17\u001b[m / \u001b[33m16122\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m9M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m3M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7d7ec9-8739-4f40-a802-82f882e84047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc45ce981ec643a1a54da4ce17476718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb713102441f47af8cdfcba13c8e5027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47e7c892721425cb14d7ba0b1125c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34557ee798844519f295e51f6ce29d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740b5bc58b82487d9444a8a7c4b17948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ise-uiuc/Magicoder-S-CL-7B\",\n",
    "                                          load_in_4bit = True\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3408e1ae-0d4f-4d8e-9ce4-11a9210d784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7b75665a5d4a618e7a31dfd18168fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/692 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b780f7d09fd24c2fac09df8127aa9500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea05c0675c84ed284fa7a3dc7718d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435e8abb59a841ac975262b335c16f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b24fd40548411fadb01e20f41d3a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9993fa75e74f4da702605846704499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42fe58cd1fa4a82b2f06fcb2f9be4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d718b3e0ae149b7be42dd3319b92cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7335c1ea4504650ae3c15e714fe854f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8849b430cc8d4df4967251225eeb74dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5348307672af4241b34ab76cf1b7847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"ise-uiuc/Magicoder-S-CL-7B\",\n",
    "                                             load_in_4bit = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ac195e-b28e-4c65-b5df-9b74f05fa48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                top_k = 1,\n",
    "                do_sample = True,\n",
    "                temperature = 0.2,\n",
    "                max_new_tokens = 200,\n",
    "                # eos_token_id = [10897,63,4686,10252],\n",
    "                \n",
    "               )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87999e2d-f231-4133-95e2-945d62b9ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = llm.invoke(\"write pandas code to calulate mean age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecbb0fe5-a289-4193-9563-1a6d81ebf82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for each group\n",
      "Zyg_mean = df.groupby('zygote').mean()\n",
      "\n",
      "# write pandas code to calculate median for each group\n",
      "Zyg_median = df.groupby('zygote').median()\n",
      "\n",
      "# write pandas code to calculate standard deviation for each group\n",
      "Zyg_std = df.groupby('zygote').std()\n",
      "\n",
      "# write pandas code to calculate minimum for each group\n",
      "Zyg_min = df.groupby('zygote').min()\n",
      "\n",
      "# write pandas code to calculate maximum for each group\n",
      "Zyg_max = df.groupby('zygote').max()\n",
      "\n",
      "# write pandas code to calculate sum for each group\n",
      "Zyg_sum = df.groupby('zygote').sum()\n",
      "\n",
      "# write pandas code to calculate count for each group\n",
      "Zyg_count = df.groupby('zygote').count()\n",
      "\n",
      "# write pandas code to calculate quantile for each group\n",
      "Zyg_quant\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayur2",
   "language": "python",
   "name": "mayur2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
